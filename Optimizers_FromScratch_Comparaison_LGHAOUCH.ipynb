{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jKDJJ0CmDj6X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras.datasets.mnist as mnist\n",
        "\n",
        "import sys\n",
        "sys.path.append('../Neural_Network_from_Scratch')\n",
        "import optimizers as opt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddpYgD7KDj6g",
        "outputId": "30a85b0b-1a5d-4d51-ef2c-e9db2de210fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load data from tensorflow\n",
        "data = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6eUe_vBiDj6h"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naama4mcDj6j",
        "outputId": "ad78cd63-6a54-43a1-b283-b79f146ff153"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Tlgd4rWJDj6k",
        "outputId": "c948020f-96db-4c3c-cd20-f1b7c3b7ec0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f584b91f2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUQF9VkSDj6m",
        "outputId": "676e66f6-f61f-43c2-eba6-b7d9abb673ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data\n",
        "# Reshape (flatten)\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(x_train.shape, x_test.shape)\n",
        "\n",
        "# Normalize data within {0,1} + dtype conversion\n",
        "x_train = np.array(x_train/255., dtype=np.float32)\n",
        "x_test = np.array(x_test/255., dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4PABy8VUDj6n"
      },
      "outputs": [],
      "source": [
        "# Function to convert labels to one-hot encodings\n",
        "def one_hot(Y):\n",
        "    num_labels = len(set(Y))\n",
        "    new_Y = []\n",
        "    for label in Y:\n",
        "        encoding = np.zeros(num_labels)\n",
        "        encoding[label] = 1.\n",
        "        new_Y.append(encoding)\n",
        "    return np.array(new_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmgHd1lJDj6o",
        "outputId": "dfc1a96f-64fa-47ea-bc35-7cc540a5f815"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (10000, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "y_train = one_hot(y_train)\n",
        "y_test = one_hot(y_test)\n",
        "y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q06NQ4EQDj6p"
      },
      "outputs": [],
      "source": [
        "# Layer object to handle weights, biases, activation (if any) of a layer\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, hidden_units: int, activation:str=None):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.activation = activation\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "        \n",
        "    def initialize_params(self, n_in, hidden_units):\n",
        "        # set seed for reproducibility \n",
        "        np.random.seed(42)\n",
        "        self.W = np.random.randn(n_in, hidden_units) * np.sqrt(2/n_in) \n",
        "        np.random.seed(42)\n",
        "        self.b = np.zeros((1, hidden_units))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.input = np.array(X, copy=True)\n",
        "        if self.W is None:\n",
        "            self.initialize_params(self.input.shape[-1], self.hidden_units)\n",
        "\n",
        "        self.Z = X @ self.W + self.b\n",
        "        \n",
        "        if self.activation is not None:\n",
        "            self.A = self.activation_fn(self.Z)\n",
        "            return self.A\n",
        "        return self.Z\n",
        "    \n",
        "    def activation_fn(self, z, derivative=False):\n",
        "        if self.activation == 'relu':\n",
        "            if derivative:\n",
        "                return self.drelu(z)\n",
        "            return self.relu(z)\n",
        "        if self.activation == 'sigmoid':\n",
        "            if derivative:\n",
        "                return self.dsigmoid(z)\n",
        "            return self.sigmoid(z)\n",
        "        if self.activation == 'softmax':\n",
        "            if derivative: \n",
        "                return self.dsoftmax(z)\n",
        "            return self.softmax(z)\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def dsigmoid(z):\n",
        "        return Layer.sigmoid(z) * (1-Layer.sigmoid(z))\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    @staticmethod\n",
        "    def drelu(z):\n",
        "        return np.where(z<=0,0,1)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):  # numerically stable version of softmax \n",
        "        exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def dsoftmax(x):\n",
        "        exp = np.exp(x - np.max(x, axis=1, keepdims=True)) \n",
        "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(f'''Hidden Units={self.hidden_units}; Activation={self.activation}''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G9JjOD9mDj6s"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.layers = dict()\n",
        "        self.cache = dict()\n",
        "        self.grads = dict()\n",
        "        \n",
        "    def add(self, layer):\n",
        "        self.layers[len(self.layers)+1] = layer\n",
        "        \n",
        "    def set_config(self, epochs, learning_rate, optimizer=None):\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        if not not self.optimizer:\n",
        "            self.optimizer.config(self.layers)\n",
        "            self.optimizer.epochs = self.epochs\n",
        "            self.optimizer.learning_rate = self.learning_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        for idx, layer in self.layers.items():\n",
        "            x = layer.forward(x)\n",
        "            self.cache[f'W{idx}'] = layer.W\n",
        "            self.cache[f'Z{idx}'] = layer.Z\n",
        "            self.cache[f'A{idx}'] = layer.A\n",
        "        return x\n",
        "\n",
        "    def backward(self, y):\n",
        "        last_layer_idx = max(self.layers.keys())\n",
        "        m = y.shape[0]\n",
        "        # back prop through all dZs \n",
        "        for idx in reversed(range(1, last_layer_idx+1)):\n",
        "            if idx == last_layer_idx:\n",
        "                # e.g. dZ3 = y_pred - y_true for a 3 layer network \n",
        "                self.grads[f'dZ{idx}'] = self.cache[f'A{idx}'] - y\n",
        "            else:\n",
        "                # dZn = dZ(n+1) dot W(n+1) * inverse derivative of activation function of Layer n, with Zn as input \n",
        "                self.grads[f'dZ{idx}'] = self.grads[f'dZ{idx+1}'] @ self.cache[f'W{idx+1}'].T *\\\n",
        "                                        self.layers[idx].activation_fn(self.cache[f'Z{idx}'], derivative=True)\n",
        "            self.grads[f'dW{idx}'] = 1 / m * self.layers[idx].input.T @ self.grads[f'dZ{idx}']\n",
        "            self.grads[f'db{idx}'] = 1 / m * np.sum(self.grads[f'dZ{idx}'], axis=0, keepdims=True)\n",
        "            \n",
        "            assert self.grads[f'dW{idx}'].shape == self.cache[f'W{idx}'].shape\n",
        "\n",
        "    def update_params(self, epoch_num, steps):\n",
        "        for idx in self.layers.keys():\n",
        "            if self.optimizer is None:\n",
        "                self.optimize(idx)\n",
        "            else:\n",
        "                self.optimizer.optimize(idx, self.layers, self.grads, epoch_num, steps)                \n",
        " \n",
        "    def optimize(self, idx):  \n",
        "        # Vanilla minibatch gradient descent\n",
        "        self.layers[idx].W -= self.learning_rate * self.grads[f'dW{idx}']\n",
        "        self.layers[idx].b -= self.learning_rate * self.grads[f'db{idx}']\n",
        "        \n",
        "    def fit(self, x_train, y_train, x_test=None, y_test=None, batch_size=32):\n",
        "        '''Training cycle of the model object'''\n",
        "        losses = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "\n",
        "        for i in range(1, self.epochs+1):\n",
        "            print(f'Epoch {i}')\n",
        "            batches = self.create_batches(x_train, y_train, batch_size)\n",
        "            epoch_loss = []\n",
        "            steps = 0\n",
        "            \n",
        "            for x, y in batches:\n",
        "                steps += 1\n",
        "                preds = self.forward(x)\n",
        "                loss = self.compute_loss(y, preds)\n",
        "                epoch_loss.append(loss)\n",
        "\n",
        "                # Backward propagation - calculation of gradients \n",
        "                self.backward(y)\n",
        "                \n",
        "                # update weights and biases of each layer\n",
        "                self.update_params(i, steps)\n",
        "                \n",
        "            loss = sum(epoch_loss) / len(epoch_loss)\n",
        "            losses.append(loss)\n",
        "\n",
        "            # Predict with network on x_train\n",
        "            train_preds = self.forward(x_train)\n",
        "            c = np.argmax(train_preds, axis=1) == np.argmax(y_train, axis=1)\n",
        "            train_acc = list(c).count(True) / len(c) * 100\n",
        "            train_accs.append(train_acc)\n",
        "            \n",
        "            # Predict with network on x_test\n",
        "\n",
        "            test_preds = self.forward(x_test)\n",
        "            c = np.argmax(test_preds, axis=1) == np.argmax(y_test, axis=1)\n",
        "            val_acc = list(c).count(True)/len(c) * 100\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            print(f'Loss:{loss} Train Acc: {train_acc} Val Acc: {val_acc}')            \n",
        "                \n",
        "        self.history = {'train_loss': losses, 'train_acc': train_accs, 'val_acc': val_accs}\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_loss(Y, Y_hat):\n",
        "        m = Y.shape[0]\n",
        "        L = -1./m * np.sum(Y * np.log(Y_hat))\n",
        "        return L\n",
        "\n",
        "    @staticmethod\n",
        "    def mse(Y, Y_hat):\n",
        "        m = Y.shape[0]\n",
        "        return np.mean((Y- Y_hat)**2)\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_batches(x, y, batch_size):\n",
        "        m = x.shape[0]\n",
        "        num_batches = m / batch_size\n",
        "        batches = []\n",
        "        for i in range(int(num_batches+1)):\n",
        "            batch_x = x[i*batch_size:(i+1)*batch_size]\n",
        "            batch_y = y[i*batch_size:(i+1)*batch_size]\n",
        "            batches.append((batch_x, batch_y))\n",
        "        \n",
        "        # without this, batch sizes that are perfectly divisible will create an \n",
        "        # empty array at index -1\n",
        "        if m % batch_size == 0:\n",
        "            batches.pop(-1)\n",
        "\n",
        "        return batches\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WnBXbAXuDj6v"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "lr_decay = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tK5ogupnDj6v"
      },
      "outputs": [],
      "source": [
        "def run_model(optimizer=None):\n",
        "    model = Model()\n",
        "    model.add(Layer(128, activation='relu'))\n",
        "    model.add(Layer(64, activation='relu'))\n",
        "    model.add(Layer(10, activation='softmax'))\n",
        "    model.set_config(epochs=epochs, learning_rate=lr, optimizer=optimizer)\n",
        "\n",
        "    model.fit(x_train, y_train, x_test, y_test, batch_size=batch_size)\n",
        "    return model.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VKyap0q_Dj6w"
      },
      "outputs": [],
      "source": [
        "# Demon stands for decaying momentum from this paper : https://arxiv.org/pdf/1910.04952v3.pdf\n",
        "# Optimizers : refer to code in link below for implementation\n",
        "# https://github.com/timvvvht/Neural-Networks-and-Optimizers-from-scratch/blob/main/optimizers.py\n",
        "\n",
        "sgdm = opt.SGDM(lr, name='SGDM')\n",
        "nesterov = opt.Nesterov(lr, name='Nesterov')\n",
        "adagrad = opt.Adagrad(lr, name='Adagrad')\n",
        "rmsprop = opt.RMSprop(lr, name='RMSprop')\n",
        "adadelta = opt.Adadelta(lr, name='Adadelta')\n",
        "adam = opt.Adam(lr, name='Adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UajO2lTDj6x"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7M_ITjydDj6y"
      },
      "outputs": [],
      "source": [
        "opts = [sgdm, nesterov, adagrad, rmsprop, adadelta, adam]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LOfBJUtDj6y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8xlW9QRDj6z",
        "outputId": "92b40c27-edff-4a99-c8a4-96cba142e304"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<optimizers.SGDM at 0x7f584b956350>,\n",
              " <optimizers.Nesterov at 0x7f5854368c50>,\n",
              " <optimizers.Adagrad at 0x7f584a822c10>,\n",
              " <optimizers.RMSprop at 0x7f584b9563d0>,\n",
              " <optimizers.Adadelta at 0x7f584a822190>,\n",
              " <optimizers.Adam at 0x7f584a811f10>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "opts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIYxtOOQDj6z",
        "outputId": "c6ab105f-ed47-4528-f383-6c89afb1776c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss:1.971548086743485 Train Acc: 60.940000000000005 Val Acc: 61.78\n",
            "Epoch 2\n",
            "Loss:1.250420248838212 Train Acc: 77.28833333333334 Val Acc: 78.38000000000001\n",
            "Epoch 3\n",
            "Loss:0.7758959326613197 Train Acc: 84.01666666666667 Val Acc: 85.24000000000001\n",
            "Epoch 4\n",
            "Loss:0.5239670911303634 Train Acc: 87.92999999999999 Val Acc: 88.99000000000001\n",
            "Epoch 5\n",
            "Loss:0.3382509386365137 Train Acc: 92.72666666666667 Val Acc: 93.05\n",
            "Epoch 6\n",
            "Loss:0.2160378233607533 Train Acc: 94.80333333333333 Val Acc: 94.69999999999999\n",
            "Epoch 7\n",
            "Loss:0.16188467669077286 Train Acc: 96.2 Val Acc: 96.0\n",
            "Epoch 8\n",
            "Loss:0.1305431330725829 Train Acc: 96.65333333333334 Val Acc: 96.34\n",
            "Epoch 9\n",
            "Loss:0.11086172850364373 Train Acc: 97.29166666666667 Val Acc: 96.71\n",
            "Epoch 10\n",
            "Loss:0.09447141691313259 Train Acc: 97.615 Val Acc: 96.84\n",
            "Epoch 11\n",
            "Loss:0.07913496700905817 Train Acc: 97.79666666666667 Val Acc: 96.81\n",
            "Epoch 12\n",
            "Loss:0.06897837086191724 Train Acc: 97.925 Val Acc: 96.74000000000001\n",
            "Epoch 13\n",
            "Loss:0.062060521020643716 Train Acc: 97.99833333333333 Val Acc: 96.85000000000001\n",
            "Epoch 14\n",
            "Loss:0.05633297340935991 Train Acc: 98.30499999999999 Val Acc: 96.97\n",
            "Epoch 15\n",
            "Loss:0.05107026027816787 Train Acc: 98.64333333333335 Val Acc: 97.33000000000001\n",
            "Epoch 16\n",
            "Loss:0.045868987622310474 Train Acc: 98.81666666666666 Val Acc: 97.56\n",
            "Epoch 17\n",
            "Loss:0.04112409724026969 Train Acc: 98.965 Val Acc: 97.50999999999999\n",
            "Epoch 18\n",
            "Loss:0.03782737159145678 Train Acc: 99.04166666666666 Val Acc: 97.58\n",
            "Epoch 19\n",
            "Loss:0.0350744310805585 Train Acc: 99.11833333333333 Val Acc: 97.6\n",
            "Epoch 20\n",
            "Loss:0.03321487363372427 Train Acc: 99.07333333333334 Val Acc: 97.65\n",
            "Epoch 1\n",
            "Loss:1.9704602353734222 Train Acc: 60.96666666666667 Val Acc: 61.809999999999995\n",
            "Epoch 2\n",
            "Loss:1.2492045883557268 Train Acc: 77.275 Val Acc: 78.41\n",
            "Epoch 3\n",
            "Loss:0.7752782565332456 Train Acc: 83.99333333333333 Val Acc: 85.2\n",
            "Epoch 4\n",
            "Loss:0.5235083758006135 Train Acc: 87.92 Val Acc: 88.92\n",
            "Epoch 5\n",
            "Loss:0.3309839506914328 Train Acc: 92.805 Val Acc: 93.25\n",
            "Epoch 6\n",
            "Loss:0.21324256120018797 Train Acc: 94.88333333333333 Val Acc: 94.82000000000001\n",
            "Epoch 7\n",
            "Loss:0.15996038924505562 Train Acc: 96.18666666666667 Val Acc: 95.88\n",
            "Epoch 8\n",
            "Loss:0.12805185070117284 Train Acc: 96.89666666666666 Val Acc: 96.41999999999999\n",
            "Epoch 9\n",
            "Loss:0.10723310548254822 Train Acc: 97.37666666666667 Val Acc: 96.69\n",
            "Epoch 10\n",
            "Loss:0.09302741390169603 Train Acc: 97.63333333333334 Val Acc: 96.84\n",
            "Epoch 11\n",
            "Loss:0.08170495425120991 Train Acc: 97.85166666666667 Val Acc: 96.99\n",
            "Epoch 12\n",
            "Loss:0.07074861171046834 Train Acc: 98.08 Val Acc: 97.11\n",
            "Epoch 13\n",
            "Loss:0.06088358675288745 Train Acc: 98.24666666666667 Val Acc: 97.06\n",
            "Epoch 14\n",
            "Loss:0.05360022706590098 Train Acc: 98.37666666666667 Val Acc: 97.05\n",
            "Epoch 15\n",
            "Loss:0.04766318330172523 Train Acc: 98.58166666666666 Val Acc: 97.25\n",
            "Epoch 16\n",
            "Loss:0.04280971332380999 Train Acc: 98.75833333333334 Val Acc: 97.39\n",
            "Epoch 17\n",
            "Loss:0.03891703171194896 Train Acc: 98.86833333333334 Val Acc: 97.50999999999999\n",
            "Epoch 18\n",
            "Loss:0.03563245884995356 Train Acc: 99.00833333333333 Val Acc: 97.54\n",
            "Epoch 19\n",
            "Loss:0.03289496956360478 Train Acc: 99.16666666666667 Val Acc: 97.7\n",
            "Epoch 20\n",
            "Loss:0.030691047175124306 Train Acc: 99.225 Val Acc: 97.72999999999999\n",
            "Epoch 1\n",
            "Loss:0.9514762519950675 Train Acc: 85.97 Val Acc: 87.3\n",
            "Epoch 2\n",
            "Loss:0.485279612503202 Train Acc: 88.715 Val Acc: 89.64\n",
            "Epoch 3\n",
            "Loss:0.40435011652071595 Train Acc: 89.75166666666667 Val Acc: 90.60000000000001\n",
            "Epoch 4\n",
            "Loss:0.36585239188264357 Train Acc: 90.41833333333334 Val Acc: 90.98\n",
            "Epoch 5\n",
            "Loss:0.3416540099021146 Train Acc: 90.94 Val Acc: 91.47\n",
            "Epoch 6\n",
            "Loss:0.3242253241997581 Train Acc: 91.27 Val Acc: 91.79\n",
            "Epoch 7\n",
            "Loss:0.3106259144425639 Train Acc: 91.57499999999999 Val Acc: 91.97\n",
            "Epoch 8\n",
            "Loss:0.29941988316962515 Train Acc: 91.83 Val Acc: 92.22\n",
            "Epoch 9\n",
            "Loss:0.28994856193572865 Train Acc: 92.085 Val Acc: 92.46\n",
            "Epoch 10\n",
            "Loss:0.2817776924594709 Train Acc: 92.28333333333333 Val Acc: 92.58999999999999\n",
            "Epoch 11\n",
            "Loss:0.27458352316899004 Train Acc: 92.47 Val Acc: 92.78\n",
            "Epoch 12\n",
            "Loss:0.2681471145107922 Train Acc: 92.64166666666667 Val Acc: 92.96\n",
            "Epoch 13\n",
            "Loss:0.26231378869958255 Train Acc: 92.785 Val Acc: 93.07\n",
            "Epoch 14\n",
            "Loss:0.2569929493961753 Train Acc: 92.93666666666667 Val Acc: 93.17999999999999\n",
            "Epoch 15\n",
            "Loss:0.2520925617788826 Train Acc: 93.035 Val Acc: 93.25\n",
            "Epoch 16\n",
            "Loss:0.24753349497390942 Train Acc: 93.16166666666666 Val Acc: 93.37\n",
            "Epoch 17\n",
            "Loss:0.2432614575322403 Train Acc: 93.27499999999999 Val Acc: 93.52000000000001\n",
            "Epoch 18\n",
            "Loss:0.239234347050478 Train Acc: 93.37333333333333 Val Acc: 93.60000000000001\n",
            "Epoch 19\n",
            "Loss:0.2354281091912523 Train Acc: 93.46333333333334 Val Acc: 93.66\n",
            "Epoch 20\n",
            "Loss:0.2318478408569799 Train Acc: 93.57 Val Acc: 93.78999999999999\n",
            "Epoch 1\n",
            "Loss:0.3494961630785522 Train Acc: 93.50666666666667 Val Acc: 93.25\n",
            "Epoch 2\n",
            "Loss:0.1642610979970541 Train Acc: 95.82333333333334 Val Acc: 95.52000000000001\n",
            "Epoch 3\n",
            "Loss:0.1268348128420613 Train Acc: 96.825 Val Acc: 96.32\n",
            "Epoch 4\n",
            "Loss:0.10772133084823718 Train Acc: 97.345 Val Acc: 96.66\n",
            "Epoch 5\n",
            "Loss:0.09654084214486329 Train Acc: 97.785 Val Acc: 96.88\n",
            "Epoch 6\n",
            "Loss:0.09060500542980882 Train Acc: 98.01666666666667 Val Acc: 97.08\n",
            "Epoch 7\n",
            "Loss:0.08587229819013464 Train Acc: 98.18666666666667 Val Acc: 97.11999999999999\n",
            "Epoch 8\n",
            "Loss:0.08136149266792374 Train Acc: 98.265 Val Acc: 96.99\n",
            "Epoch 9\n",
            "Loss:0.07906671621908073 Train Acc: 98.34666666666666 Val Acc: 97.03\n",
            "Epoch 10\n",
            "Loss:0.07584279442063008 Train Acc: 98.34166666666667 Val Acc: 96.89\n",
            "Epoch 11\n",
            "Loss:0.07550684015502815 Train Acc: 98.53333333333333 Val Acc: 97.15\n",
            "Epoch 12\n",
            "Loss:0.07328848989900642 Train Acc: 98.65166666666667 Val Acc: 97.24000000000001\n",
            "Epoch 13\n",
            "Loss:0.07187550076284573 Train Acc: 98.65 Val Acc: 97.09\n",
            "Epoch 14\n",
            "Loss:0.06932650306896895 Train Acc: 98.76833333333333 Val Acc: 97.28\n",
            "Epoch 15\n",
            "Loss:0.06722342516249417 Train Acc: 98.83166666666666 Val Acc: 97.26\n",
            "Epoch 16\n",
            "Loss:0.06571027316514047 Train Acc: 98.83999999999999 Val Acc: 97.3\n",
            "Epoch 17\n",
            "Loss:0.06514683818249663 Train Acc: 98.86666666666667 Val Acc: 97.16\n",
            "Epoch 18\n",
            "Loss:0.06022857240926718 Train Acc: 98.87833333333333 Val Acc: 97.28\n",
            "Epoch 19\n",
            "Loss:0.05749305775102681 Train Acc: 98.94500000000001 Val Acc: 97.22\n",
            "Epoch 20\n",
            "Loss:0.056519590728721945 Train Acc: 98.85833333333333 Val Acc: 97.15\n",
            "Epoch 1\n",
            "Loss:0.5122931164125435 Train Acc: 91.13 Val Acc: 91.72\n",
            "Epoch 2\n",
            "Loss:0.24122521956614748 Train Acc: 93.63833333333334 Val Acc: 93.64\n",
            "Epoch 3\n",
            "Loss:0.19096595483785975 Train Acc: 94.87666666666667 Val Acc: 94.75\n",
            "Epoch 4\n",
            "Loss:0.16173750372849793 Train Acc: 95.67 Val Acc: 95.62\n",
            "Epoch 5\n",
            "Loss:0.14253264505955782 Train Acc: 96.12833333333334 Val Acc: 96.00999999999999\n",
            "Epoch 6\n",
            "Loss:0.12921433866314433 Train Acc: 96.54333333333334 Val Acc: 96.25\n",
            "Epoch 7\n",
            "Loss:0.11944756566132668 Train Acc: 96.82166666666666 Val Acc: 96.46000000000001\n",
            "Epoch 8\n",
            "Loss:0.11144714572905381 Train Acc: 97.045 Val Acc: 96.58\n",
            "Epoch 9\n",
            "Loss:0.10507834269378703 Train Acc: 97.20666666666666 Val Acc: 96.69\n",
            "Epoch 10\n",
            "Loss:0.099982107963387 Train Acc: 97.32666666666667 Val Acc: 96.78\n",
            "Epoch 11\n",
            "Loss:0.09579553266463087 Train Acc: 97.41499999999999 Val Acc: 96.85000000000001\n",
            "Epoch 12\n",
            "Loss:0.0922189715158291 Train Acc: 97.54 Val Acc: 96.89\n",
            "Epoch 13\n",
            "Loss:0.08888134116143141 Train Acc: 97.65833333333333 Val Acc: 96.94\n",
            "Epoch 14\n",
            "Loss:0.08585790812846789 Train Acc: 97.725 Val Acc: 97.02\n",
            "Epoch 15\n",
            "Loss:0.08304233492551624 Train Acc: 97.81 Val Acc: 97.05\n",
            "Epoch 16\n",
            "Loss:0.08087862017004661 Train Acc: 97.87 Val Acc: 97.1\n",
            "Epoch 17\n",
            "Loss:0.07904081253451778 Train Acc: 97.92 Val Acc: 97.14\n",
            "Epoch 18\n",
            "Loss:0.07742831208009779 Train Acc: 97.98833333333333 Val Acc: 97.17\n",
            "Epoch 19\n",
            "Loss:0.07582162127056265 Train Acc: 98.05 Val Acc: 97.16\n",
            "Epoch 20\n",
            "Loss:0.07447354000926491 Train Acc: 98.08166666666666 Val Acc: 97.18\n",
            "Epoch 1\n",
            "Loss:0.3462405933309277 Train Acc: 94.435 Val Acc: 94.15\n",
            "Epoch 2\n",
            "Loss:0.14825623941078148 Train Acc: 96.23833333333334 Val Acc: 95.71\n",
            "Epoch 3\n",
            "Loss:0.11145417180169577 Train Acc: 97.12833333333334 Val Acc: 96.46000000000001\n",
            "Epoch 4\n",
            "Loss:0.08935200043307787 Train Acc: 97.67166666666667 Val Acc: 96.72\n",
            "Epoch 5\n",
            "Loss:0.07398113188749979 Train Acc: 98.00666666666666 Val Acc: 96.91\n",
            "Epoch 6\n",
            "Loss:0.06255578133234584 Train Acc: 98.21833333333333 Val Acc: 97.06\n",
            "Epoch 7\n",
            "Loss:0.05367491966493914 Train Acc: 98.42 Val Acc: 97.18\n",
            "Epoch 8\n",
            "Loss:0.04643268179420983 Train Acc: 98.58666666666667 Val Acc: 97.24000000000001\n",
            "Epoch 9\n",
            "Loss:0.0401502688351156 Train Acc: 98.67 Val Acc: 97.26\n",
            "Epoch 10\n",
            "Loss:0.03492799950511687 Train Acc: 98.7 Val Acc: 97.27\n",
            "Epoch 11\n",
            "Loss:0.030460472903013906 Train Acc: 98.89666666666666 Val Acc: 97.35000000000001\n",
            "Epoch 12\n",
            "Loss:0.026486564063315145 Train Acc: 98.96000000000001 Val Acc: 97.32\n",
            "Epoch 13\n",
            "Loss:0.02301276499963523 Train Acc: 99.21833333333333 Val Acc: 97.46000000000001\n",
            "Epoch 14\n",
            "Loss:0.019850895986943794 Train Acc: 99.27333333333334 Val Acc: 97.52\n",
            "Epoch 15\n",
            "Loss:0.0170929401643036 Train Acc: 99.345 Val Acc: 97.55\n",
            "Epoch 16\n",
            "Loss:0.014669258212178984 Train Acc: 99.36333333333334 Val Acc: 97.57000000000001\n",
            "Epoch 17\n",
            "Loss:0.012637270429251227 Train Acc: 99.38166666666667 Val Acc: 97.63\n",
            "Epoch 18\n",
            "Loss:0.010859550462818078 Train Acc: 99.43333333333332 Val Acc: 97.59\n",
            "Epoch 19\n",
            "Loss:0.009267488182499603 Train Acc: 99.47166666666666 Val Acc: 97.67\n",
            "Epoch 20\n",
            "Loss:0.007982175195849461 Train Acc: 99.55666666666667 Val Acc: 97.65\n"
          ]
        }
      ],
      "source": [
        "opts_history = {i.name: run_model(i) for i in opts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKNkJ5QmDj60",
        "outputId": "3c8b09ca-3f56-446d-effe-058147ff239d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss:2.1503119682928302 Train Acc: 43.27166666666667 Val Acc: 43.63\n",
            "Epoch 2\n",
            "Loss:1.7890254011275981 Train Acc: 60.99166666666667 Val Acc: 61.86000000000001\n",
            "Epoch 3\n",
            "Loss:1.4460175021900223 Train Acc: 69.90666666666667 Val Acc: 70.96000000000001\n",
            "Epoch 4\n",
            "Loss:1.1681158313491378 Train Acc: 75.38833333333334 Val Acc: 76.83\n",
            "Epoch 5\n",
            "Loss:0.9724806898954285 Train Acc: 78.97333333333333 Val Acc: 80.05\n",
            "Epoch 6\n",
            "Loss:0.8376267271694035 Train Acc: 81.15833333333333 Val Acc: 82.41000000000001\n",
            "Epoch 7\n",
            "Loss:0.7421259956904581 Train Acc: 82.87166666666667 Val Acc: 84.07\n",
            "Epoch 8\n",
            "Loss:0.6719898616919099 Train Acc: 84.08 Val Acc: 85.34\n",
            "Epoch 9\n",
            "Loss:0.6186536204666072 Train Acc: 85.055 Val Acc: 86.32\n",
            "Epoch 10\n",
            "Loss:0.5768878117558198 Train Acc: 85.79166666666667 Val Acc: 86.89\n",
            "Epoch 11\n",
            "Loss:0.5434002606364619 Train Acc: 86.40833333333333 Val Acc: 87.41\n",
            "Epoch 12\n",
            "Loss:0.5160244313197697 Train Acc: 86.87 Val Acc: 87.92999999999999\n",
            "Epoch 13\n",
            "Loss:0.4932644358199432 Train Acc: 87.29666666666667 Val Acc: 88.37\n",
            "Epoch 14\n",
            "Loss:0.4740456128150996 Train Acc: 87.63 Val Acc: 88.69\n",
            "Epoch 15\n",
            "Loss:0.45761008849869933 Train Acc: 87.91 Val Acc: 89.03\n",
            "Epoch 16\n",
            "Loss:0.4434002522618662 Train Acc: 88.205 Val Acc: 89.21\n",
            "Epoch 17\n",
            "Loss:0.4309912004325757 Train Acc: 88.42 Val Acc: 89.38000000000001\n",
            "Epoch 18\n",
            "Loss:0.4200562273562606 Train Acc: 88.62833333333333 Val Acc: 89.53999999999999\n",
            "Epoch 19\n",
            "Loss:0.4103384811479114 Train Acc: 88.84 Val Acc: 89.73\n",
            "Epoch 20\n",
            "Loss:0.40162777020010226 Train Acc: 89.04166666666666 Val Acc: 89.92\n"
          ]
        }
      ],
      "source": [
        "opts_history['Gradient_Descent'] = (run_model(None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nzqJ_mKDj61",
        "outputId": "cbfa2afe-aa37-404d-8a6f-6429f98d3025"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<optimizers.SGDM at 0x7f584b956350>,\n",
              " <optimizers.Nesterov at 0x7f5854368c50>,\n",
              " <optimizers.Adagrad at 0x7f584a822c10>,\n",
              " <optimizers.RMSprop at 0x7f584b9563d0>,\n",
              " <optimizers.Adadelta at 0x7f584a822190>,\n",
              " <optimizers.Adam at 0x7f584a811f10>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "opts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "dQZlksJqDj61",
        "outputId": "9ec753c2-164a-4fc9-90cc-a2289f0cec6d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"352bb533-d408-4448-8fde-2aefde43ff17\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"352bb533-d408-4448-8fde-2aefde43ff17\")) {                    Plotly.newPlot(                        \"352bb533-d408-4448-8fde-2aefde43ff17\",                        [{\"name\":\"Gradient_Descent\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[2.1503119682928302,1.7890254011275981,1.4460175021900223,1.1681158313491378,0.9724806898954285,0.8376267271694035,0.7421259956904581,0.6719898616919099,0.6186536204666072,0.5768878117558198,0.5434002606364619,0.5160244313197697,0.4932644358199432,0.4740456128150996,0.45761008849869933,0.4434002522618662,0.4309912004325757,0.4200562273562606,0.4103384811479114,0.40162777020010226],\"type\":\"scatter\"},{\"name\":\"SGDM\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[1.971548086743485,1.250420248838212,0.7758959326613197,0.5239670911303634,0.3382509386365137,0.2160378233607533,0.16188467669077286,0.1305431330725829,0.11086172850364373,0.09447141691313259,0.07913496700905817,0.06897837086191724,0.062060521020643716,0.05633297340935991,0.05107026027816787,0.045868987622310474,0.04112409724026969,0.03782737159145678,0.0350744310805585,0.03321487363372427],\"type\":\"scatter\"},{\"name\":\"Nesterov\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[1.9704602353734222,1.2492045883557268,0.7752782565332456,0.5235083758006135,0.3309839506914328,0.21324256120018797,0.15996038924505562,0.12805185070117284,0.10723310548254822,0.09302741390169603,0.08170495425120991,0.07074861171046834,0.06088358675288745,0.05360022706590098,0.04766318330172523,0.04280971332380999,0.03891703171194896,0.03563245884995356,0.03289496956360478,0.030691047175124306],\"type\":\"scatter\"},{\"name\":\"RMSprop\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[0.3494961630785522,0.1642610979970541,0.1268348128420613,0.10772133084823718,0.09654084214486329,0.09060500542980882,0.08587229819013464,0.08136149266792374,0.07906671621908073,0.07584279442063008,0.07550684015502815,0.07328848989900642,0.07187550076284573,0.06932650306896895,0.06722342516249417,0.06571027316514047,0.06514683818249663,0.06022857240926718,0.05749305775102681,0.056519590728721945],\"type\":\"scatter\"},{\"name\":\"Adam\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[0.3462405933309277,0.14825623941078148,0.11145417180169577,0.08935200043307787,0.07398113188749979,0.06255578133234584,0.05367491966493914,0.04643268179420983,0.0401502688351156,0.03492799950511687,0.030460472903013906,0.026486564063315145,0.02301276499963523,0.019850895986943794,0.0170929401643036,0.014669258212178984,0.012637270429251227,0.010859550462818078,0.009267488182499603,0.007982175195849461],\"type\":\"scatter\"},{\"name\":\"Adagrad\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[0.9514762519950675,0.485279612503202,0.40435011652071595,0.36585239188264357,0.3416540099021146,0.3242253241997581,0.3106259144425639,0.29941988316962515,0.28994856193572865,0.2817776924594709,0.27458352316899004,0.2681471145107922,0.26231378869958255,0.2569929493961753,0.2520925617788826,0.24753349497390942,0.2432614575322403,0.239234347050478,0.2354281091912523,0.2318478408569799],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Loss\"},\"xaxis\":{\"title\":{\"text\":\"Epochs\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('352bb533-d408-4448-8fde-2aefde43ff17');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "\n",
        "num_epochs = [i for i in range(epochs)]\n",
        "fig = go.Figure(layout_title_text=\"Loss\")\n",
        "\n",
        "lst = ['Gradient_Descent', 'SGDM', 'Nesterov', 'RMSprop', 'Adam', 'Adagrad']\n",
        "\n",
        "\n",
        "for i in lst:\n",
        "    fig.add_trace(go.Scatter(x=num_epochs, y=opts_history[i]['train_loss'], name=i))\n",
        "    \n",
        "fig.update_xaxes(title_text='Epochs')\n",
        "fig.update_yaxes(title_text='Loss')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "OSxOICEbDj62",
        "outputId": "d21e2b49-543b-4340-d54f-3e4d46fb6cfe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"0ae4cae5-865c-43f9-b6c7-459b1b28c1d4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0ae4cae5-865c-43f9-b6c7-459b1b28c1d4\")) {                    Plotly.newPlot(                        \"0ae4cae5-865c-43f9-b6c7-459b1b28c1d4\",                        [{\"name\":\"Gradient_Descent\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[43.63,61.86000000000001,70.96000000000001,76.83,80.05,82.41000000000001,84.07,85.34,86.32,86.89,87.41,87.92999999999999,88.37,88.69,89.03,89.21,89.38000000000001,89.53999999999999,89.73,89.92],\"type\":\"scatter\"},{\"name\":\"SGDM\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[61.78,78.38000000000001,85.24000000000001,88.99000000000001,93.05,94.69999999999999,96.0,96.34,96.71,96.84,96.81,96.74000000000001,96.85000000000001,96.97,97.33000000000001,97.56,97.50999999999999,97.58,97.6,97.65],\"type\":\"scatter\"},{\"name\":\"Nesterov\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[61.809999999999995,78.41,85.2,88.92,93.25,94.82000000000001,95.88,96.41999999999999,96.69,96.84,96.99,97.11,97.06,97.05,97.25,97.39,97.50999999999999,97.54,97.7,97.72999999999999],\"type\":\"scatter\"},{\"name\":\"RMSprop\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[93.25,95.52000000000001,96.32,96.66,96.88,97.08,97.11999999999999,96.99,97.03,96.89,97.15,97.24000000000001,97.09,97.28,97.26,97.3,97.16,97.28,97.22,97.15],\"type\":\"scatter\"},{\"name\":\"Adam\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[94.15,95.71,96.46000000000001,96.72,96.91,97.06,97.18,97.24000000000001,97.26,97.27,97.35000000000001,97.32,97.46000000000001,97.52,97.55,97.57000000000001,97.63,97.59,97.67,97.65],\"type\":\"scatter\"},{\"name\":\"Adagrad\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[87.3,89.64,90.60000000000001,90.98,91.47,91.79,91.97,92.22,92.46,92.58999999999999,92.78,92.96,93.07,93.17999999999999,93.25,93.37,93.52000000000001,93.60000000000001,93.66,93.78999999999999],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Validation Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"Epochs\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0ae4cae5-865c-43f9-b6c7-459b1b28c1d4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = go.Figure(layout_title_text=\"Validation Accuracy\")\n",
        "\n",
        "for i in lst:\n",
        "    fig.add_trace(go.Scatter(x=num_epochs, y=opts_history[i]['val_acc'], name=i))\n",
        "    \n",
        "fig.update_xaxes(title_text='Epochs')\n",
        "fig.update_yaxes(title_text='Accuracy')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XG6p6rrDj63"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}